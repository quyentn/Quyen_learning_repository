{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is not for competition purpose but for self-learning purpose. I followed exactly what this guy did\n",
    "https://www.kaggle.com/thousandvoices/simple-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#print('https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version')\n",
    "print('https://www.kaggle.com/nholloway/how-word-embeddings-influence-bias')\n",
    "print('https://www.kaggle.com/thousandvoices/simple-lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> # Define some parameters for deep learning models\n",
    "EMBEDDING_FILES = [\n",
    "    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n",
    "    '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "]\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]\n",
    "AUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create build_matrix function to import embedding from text files:**\n",
    "\n",
    "*- Build_matrix: *\n",
    "\n",
    "input:  array of tuple (word, index)\n",
    "\n",
    "output: embedding_matrix is an array with shape of (number of words, dimension of embedding)\n",
    "\n",
    "*- load_embeddings:*\n",
    "\n",
    "input: path to embedding files\n",
    "\n",
    "output: dictionary with each word has a proper numpy array having shape of (number of words, dimension of embedding)\n",
    "\n",
    "*- get_coefs:*\n",
    "\n",
    "input: list or array of embedding numbers\n",
    "\n",
    "output: proper numpy array of embedding numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #Define functions to load embedding\n",
    "def get_coefs(word, *arr): #return proper numpy array to input into embedding matrix\n",
    "    return word, np.asarray(arr, dtype='float32') \n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Deep Learning model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #Build model\n",
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "    words = Input(shape=(None,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PREPROCESSING__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #Create data for model\n",
    "train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "\n",
    "x_train = train_df[TEXT_COLUMN].astype(str)\n",
    "y_train = train_df[TARGET_COLUMN].values\n",
    "y_aux_train = train_df[AUX_COLUMNS].values\n",
    "x_test = test_df[TEXT_COLUMN].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
      "['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
      "comment_text\n"
     ]
    }
   ],
   "source": [
    "print(IDENTITY_COLUMNS)\n",
    "print(AUX_COLUMNS)\n",
    "print(TEXT_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59859</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>ur a sh*tty comment.</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59861</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>hahahahahahahahhha suck it.</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>239607</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>Yet call out all Muslims for the acts of a few...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>239612</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>This bitch is nuts. Who would read a book by a...</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    target   ...      insult  threat\n",
       "4    59856  0.893617   ...    0.872340  0.0000\n",
       "5    59859  0.666667   ...    0.333333  0.0000\n",
       "6    59861  0.457627   ...    0.254237  0.0000\n",
       "31  239607  0.912500   ...    0.887500  0.1125\n",
       "34  239612  0.830769   ...    0.830769  0.0000\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train.columns\n",
    "train_df[train_df.severe_toxicity > 0][['id','target','comment_text','severe_toxicity','obscene','identity_attack','insult','threat']].head()\n",
    "#View full comments without truncation comment_list = train.comment_text.tolist() comment_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #tokenize and create weights for each record\n",
    "for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n",
    "    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n",
    "\n",
    "tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "#replace each word/token by a number and put the record in a list/sequence of numbers\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "#Add zero or truncated left part of the record to make all records with equal length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "#preset all weight of 1\n",
    "sample_weights = np.ones(len(x_train), dtype=np.float32)\n",
    "#add number of identity col\n",
    "sample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\n",
    "#only keep weight of identity columns when target is 1\n",
    "sample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\n",
    "#add 5 times weight of identity columns when target is 0\n",
    "sample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\n",
    "#normalize weight by dividing to its mean\n",
    "sample_weights /= sample_weights.mean()\n",
    "\n",
    "#concat 2 embedding matrix of num_word,300 to create a matrix of num_word,600 shape\n",
    "embedding_matrix = np.concatenate(\n",
    "    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last word: [('uv4j7sid3pk', 327821)]\n",
      "all words: 327821\n",
      "unknown words: 187282\n",
      "known words: 140540\n"
     ]
    }
   ],
   "source": [
    "print('last word: '+str([(word, i) for (word, i) in tokenizer.word_index.items() if i == len(tokenizer.word_index.items())]))\n",
    "print('all words: '+str(len(tokenizer.word_index.items())))\n",
    "a = np.zeros((600,))\n",
    "print('unknown words: '+str(len([x for x in embedding_matrix if x.all() == a.all()])))\n",
    "print('known words: '+str(len([x for x in embedding_matrix if x.all() != a.all()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_aux_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 600)    196693200   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, None, 600)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 256)    747520      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 256)    395264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 512)          0           concatenate_1[0][0]              \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 512)          0           add_1[0][0]                      \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            513         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            3078        add_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 198,364,887\n",
      "Trainable params: 1,671,687\n",
      "Non-trainable params: 196,693,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1\n",
      " - 765s - loss: 0.5260 - dense_7_loss: 0.4194 - dense_8_loss: 0.1066\n",
      "Epoch 1/1\n",
      " - 762s - loss: 0.5079 - dense_7_loss: 0.4058 - dense_8_loss: 0.1021\n",
      "Epoch 1/1\n",
      " - 764s - loss: 0.5022 - dense_7_loss: 0.4010 - dense_8_loss: 0.1012\n",
      "Epoch 1/1\n",
      " - 768s - loss: 0.4984 - dense_7_loss: 0.3978 - dense_8_loss: 0.1006\n",
      "Epoch 1/1\n",
      " - 767s - loss: 0.5258 - dense_11_loss: 0.4193 - dense_12_loss: 0.1065\n",
      "Epoch 1/1\n",
      " - 766s - loss: 0.5077 - dense_11_loss: 0.4056 - dense_12_loss: 0.1021\n",
      "Epoch 1/1\n",
      " - 767s - loss: 0.5020 - dense_11_loss: 0.4008 - dense_12_loss: 0.1012\n",
      "Epoch 1/1\n",
      " - 766s - loss: 0.4982 - dense_11_loss: 0.3976 - dense_12_loss: 0.1006\n"
     ]
    }
   ],
   "source": [
    ">>> #\n",
    "checkpoint_predictions = []\n",
    "weights = []\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            [y_train, y_aux_train],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=1,\n",
    "            verbose=2,\n",
    "            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n",
    "            callbacks=[\n",
    "                LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n",
    "            ]\n",
    "        )\n",
    "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
    "        weights.append(2 ** global_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> #Create submission file\n",
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_df.id,\n",
    "    'prediction': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Find maximum number of words (tokens) per records in the train data\n",
    "x_train1 = dt_preprocess(train['comment_text'])\n",
    "x_train1 = tokenizer.texts_to_sequences(x_train1)\n",
    "[[i,len(x)] for i,x in enumerate(x_train1) if len(x)>220]\n",
    "#Some entries have length greater than the assigned MAX_LEN\n",
    "#Any entries with length shorter than 220, the begining part will be added by 0\n",
    "#ny entries with length greater than 220, the begining part will be truncated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
