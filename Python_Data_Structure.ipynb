{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Access Memory\n",
    "- Binary Numbers\n",
    "- Fixed-Width Integers\n",
    "- Arrays\n",
    "- Strings\n",
    "- Pointers\n",
    "- Dynamic Arrays\n",
    "- Linked Lists\n",
    "- Hash Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "- Arrays have O(1)time lookups. But you need enough uninterrupted space in RAM to store the whole array. And the array items need to be the same size.\n",
    "\n",
    "- But if your array stores pointers to the actual array items (like we did with our list of baby names), you can get around both those weaknesses. You can store each array item wherever there's space in RAM, and the array items can be different sizes. The tradeoff is that now your array is slower because it's not cache-friendly.\n",
    "\n",
    "- Another problem with arrays is you have to specify their sizes ahead of time. There are two ways to get around this: dynamic arrays and linked lists. Linked lists have faster appends and prepends than dynamic arrays, but dynamic arrays have faster lookups.\n",
    "\n",
    "- Fast lookups are really useful, especially if you can look things up not just by indices (0, 1, 2, 3, etc.) but by arbitrary keys (\"lies\", \"foes\"...any string). That's what hash tables are for. The only problem with hash tables is they have to deal with hash collisions, which means some lookups could be a bit slow.\n",
    "\n",
    "- Each data structure has tradeoffs. You can't have it all. So you have to know what's important in the problem you're working on. What does your data structure need to do quickly? Is it lookups by index? Is it appends or prepends? Once you know what's important, you can pick the data structure that does it best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Array\n",
    "Some languages (including Python) don't have these bare-bones arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst Case\n",
    "- __space__:  O(n)\n",
    "- __lookup__:  O(1)\n",
    "- __append__:  O(1)\n",
    "- __insert__:  O(n)\n",
    "- __delete__:  O(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dynamic Array\n",
    "Other names: array list, growable array, resizable array, mutable array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Case \t\n",
    "- __space__: \tO(n) \n",
    "- __lookup__: \tO(1)\n",
    "- __append__: \tO(1)\n",
    "- __insert__: \tO(n)\n",
    "- __delete__: \tO(n)\n",
    "\n",
    "\n",
    " Worst Case:\n",
    "- __space__: \tO(n)\n",
    "- __lookup__: \tO(1)\n",
    "- __append__: \tO(n)\n",
    "- __insert__: \tO(n)\n",
    "- __delete__: \tO(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hash Table\n",
    " In Python 3.6, hash tables are called dictionaries. Sets copy the implementation of Dictionary in Python but they only store key without value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average \n",
    "- __space__: O(n) \n",
    "- __insert__: O(1) \n",
    "- __lookup__: O(1) \n",
    "- __delete__: O(1) \n",
    "\n",
    "    Worst Case\n",
    "- __space__: O(n)\n",
    "- __insert__: O(n)\n",
    "- __lookup__: O(n)\n",
    "- __delete__: O(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linked List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst Case\n",
    "- __space__: O(n)\n",
    "- __prepend__: O(1)\n",
    "- __append__: O(1)\n",
    "- __lookup__: O(n)\n",
    "- __insert__: O(n)\n",
    "- __delete__: O(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Most languages (including Python 3.6) don't provide a linked list implementation. Assuming we've already implemented our own, here's how we'd construct the linked list above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedListNode(object):\n",
    "    def __init__(self,val):\n",
    "        self.key = val\n",
    "        self.next = None\n",
    "\n",
    "a = LinkedListNode(5)\n",
    "b = LinkedListNode(1)\n",
    "c = LinkedListNode(9)\n",
    "\n",
    "a.next = b\n",
    "b.next = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Doubly Linked Lists__\n",
    "\n",
    "In a basic linked list, each item stores a single pointer to the next element. In a doubly linked list, items have pointers to the next and the previous nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleLinkedListNode(object):\n",
    "    def __init__(self,val):\n",
    "        self.key = val\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "\n",
    "a = DoubleLinkedListNode(5)\n",
    "b = DoubleLinkedListNode(1)\n",
    "c = DoubleLinkedListNode(9)\n",
    "\n",
    "a.next = b\n",
    "b.prev = a\n",
    "b.next = c\n",
    "c.prev = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst Case\n",
    "- __space__: O(n)\n",
    "- __enqueue__: O(1)\n",
    "- __dequeue__: O(1)\n",
    "- __peek__: O(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Implementation__\n",
    "\n",
    "Queues are easy to implement with linked lists:\n",
    "- To enqueue, insert at the tail of the linked list.\n",
    "- To dequeue, remove at the head of the linked list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Stack\n",
    "A stack stores items in a last-in, first-out (LIFO) order. You can implement a stack with either a linked list or a dynamic array—they both work pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst Case\n",
    "- __space__:O(n)\n",
    "- __push__:O(1)\n",
    "- __pop__:O(1)\n",
    "- __peek__:O(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Binary Tree \n",
    "A binary tree is a tree where every node has two or fewer children. The children are usually called left and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryTreeNode(object):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.left  = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Property 1: the number of total nodes on each \"level\" doubles as we move down the tree. \n",
    "- Property 2: the number of nodes on the last level is equal to the sum of the number of nodes on all other levels (plus 1)\n",
    "\n",
    "    - Level 0: 2<sup>0</sup> nodes,\n",
    "    - Level 1: 2<sup>1</sup> nodes,\n",
    "    - Level 2: 2<sup>2</sup> nodes,\n",
    "    - Level 3: 2<sup>3</sup> nodes,\n",
    "    - etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Graph\n",
    "- A graph organizes items in an interconnected network. \n",
    "- Most graph algorithms are O(n∗lg(n) or even slower. Depending on the size of your graph, running algorithms across your nodes may not be feasible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Edge list:__ A list of all the edges in the graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since node 3 has edges to nodes 1 and 2, [1, 3] and [2, 3] are in the edge list. \n",
    "graph = [[0, 1], [1, 2], [1, 3], [2, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Adjacency list:__ A list where the index represents the node and the value at that index is a list of the node's neighbors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since node 3 has edges to nodes 1 and 2, graph[3] has the adjacency list [1, 2]. \n",
    "graph = [[1],\n",
    "         [0, 2, 3],\n",
    "         [1, 3],\n",
    "         [1, 2],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = {0: [1],\n",
    "         1: [0, 2, 3],\n",
    "         2: [1, 3],\n",
    "         3: [1, 2],}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Adjacency matrix:__  A matrix of 0s and 1s indicating whether node x connects to node y (0 means no, 1 means yes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since node 3 has edges to nodes 1 and 2, graph[3][1] and graph[3][2] have value 1. \n",
    "graph = [[0, 1, 0, 0],\n",
    "         [1, 0, 1, 1],\n",
    "         [0, 1, 0, 1],\n",
    "         [0, 1, 1, 0],]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Algorithms__\n",
    "\n",
    "___BFS and DFS___\n",
    "- Lots of graph problems can be solved using just these traversals:\n",
    "- Is there a path between two nodes in this undirected graph? Run DFS or BFS from one node and see if you reach the other one.\n",
    "- What's the shortest path between two nodes in this undirected, unweighted graph? Run BFS from one node and backtrack once you reach the second. Note: BFS always finds the shortest path, assuming the graph is undirected and unweighted. DFS does not always find the shortest path.\n",
    "- Can this undirected graph be colored with two colors? Run BFS, assigning colors as nodes are visited. Abort if we ever try to assign a node a color different from the one it was assigned earlier.\n",
    "- Does this undirected graph have a cycle? Run BFS, keeping track of the number of times we're visiting each node. If we ever visit a node twice, then we have a cycle. \n",
    "\n",
    "___Advanced graph algorithms___\n",
    "- __Dijkstra's Algorithm:__ Finds the shortest path from one node to all other nodes in a weighted graph.\n",
    "- __Topological Sort:__ Arranges the nodes in a directed, acyclic graph in a special order based on incoming edges.\n",
    "- __Minimum Spanning Tree:__ Finds the cheapest set of edges needed to reach all nodes in a weighted graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST PROBLEMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O(nlgn) is the time to beat. Even if our list of scores were already sorted we'd have to do a full walk through the list to confirm that it was in fact fully sorted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91, 89, 65, 53, 41, 37, 37]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsorted_scores = [37,37, 89, 41, 65, 91, 53]\n",
    "HIGHEST_POSSIBLE_SCORE = 100\n",
    "def sort_scores(unsorted_scores, highest_possible_score):\n",
    "    # List of 0s at indices 0..highest_possible_score\n",
    "    score_counts = [0] * (highest_possible_score+1)\n",
    "\n",
    "    # Populate score_counts\n",
    "    for score in unsorted_scores:\n",
    "        score_counts[score] += 1\n",
    "\n",
    "    # Populate the final sorted list\n",
    "    sorted_scores = []\n",
    "\n",
    "    # For each item in score_counts\n",
    "    for score in range(len(score_counts) - 1, -1, -1):\n",
    "        count = score_counts[score]\n",
    "        sorted_scores += [score]*count\n",
    "\n",
    "    return sorted_scores\n",
    "\n",
    "sort_scores(unsorted_scores,HIGHEST_POSSIBLE_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', ':', 48), ('1', ':', 49), ('2', ':', 50), ('3', ':', 51), ('4', ':', 52), ('5', ':', 53), ('6', ':', 54), ('7', ':', 55), ('8', ':', 56), ('9', ':', 57)]\n",
      "627830183787003738979638778171212515677536395487409726836477465973329282582\n",
      "627830183787003738979638778171212515677536395487409726836477465973329282582\n"
     ]
    }
   ],
   "source": [
    "#number_one = \"193283492420348904832902348908239048823480823\"\n",
    "#number_two = \"3248234890238902348823940990234\"\n",
    "\n",
    "#Question:\n",
    "#1) I need to multiply this and get the answer\n",
    "#2) DO NOT CONVERT TO INT AND DO THE MULTIPLICATION\n",
    "\n",
    "#ord return an integer representing the Unicode code point of the character\n",
    "print([(char,':',ord(char))for char in '0123456789'])s\n",
    "\n",
    "def string_multiplication(a, b):\n",
    "    result = 0\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            result += (ord(a[i])-ord('0'))*(10**(len(a)-i-1))*(ord(b[j])-ord('0'))*(10**(len(b)-j-1))\n",
    "    return result\n",
    "\n",
    "number_one = 193283492420348904832902348908239048823480823\n",
    "number_two = 3248234890238902348823940990234\n",
    "print(number_one*number_two)\n",
    "print(string_multiplication(str(number_one), str(number_two)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Write a function that returns a list of all the duplicate files. We'll check them by hand before actually deleting them, since programmatically deleting files is really scary. To help us confirm that two files are actually duplicates, return a list of tuples ↴ where:\n",
    "\n",
    "    the first item is the duplicate file\n",
    "    the second item is the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_files(starting_directory):\n",
    "    files_seen_already = {}\n",
    "    stack = [starting_directory]\n",
    "\n",
    "    # We'll track tuples of (duplicate_file, original_file)\n",
    "    duplicates = []\n",
    "\n",
    "    while len(stack) > 0:\n",
    "        current_path = stack.pop()\n",
    "\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_duplicate_files(starting_directory):\n",
    "    files_seen_already = {}\n",
    "    stack = [starting_directory]\n",
    "\n",
    "    # We'll track tuples of (duplicate_file, original_file)\n",
    "    duplicates = []\n",
    "\n",
    "    while len(stack) > 0:\n",
    "        current_path = stack.pop()\n",
    "\n",
    "        # If it's a directory, put the contents in our stack\n",
    "        if os.path.isdir(current_path):\n",
    "            for path in os.listdir(current_path):\n",
    "                full_path = os.path.join(current_path, path)\n",
    "                stack.append(full_path)\n",
    "\n",
    "        # If it's a file\n",
    "        else:\n",
    "            # Get its contents\n",
    "            with open(current_path) as file:\n",
    "                file_contents = file.read()\n",
    "\n",
    "            # Get its last edited time\n",
    "            current_last_edited_time = os.path.getmtime(current_path)\n",
    "\n",
    "            # If we've seen it before\n",
    "            if file_contents in files_seen_already:\n",
    "                existing_last_edited_time, existing_path = files_seen_already[file_contents]\n",
    "                if current_last_edited_time > existing_last_edited_time:\n",
    "                    # Current file is the dupe!\n",
    "                    duplicates.append((current_path, existing_path))\n",
    "                else:\n",
    "                    # Old file is the dupe! So delete it\n",
    "                    duplicates.append((existing_path, current_path))\n",
    "                    # But also update files_seen_already to have the new file's info\n",
    "                    files_seen_already[file_contents] = (current_last_edited_time, current_path)\n",
    "\n",
    "            # If it's a new file, throw it in files_seen_already and record the path and the last edited time,\n",
    "            # so we can delete it later if it's a dupe\n",
    "            else:\n",
    "                files_seen_already[file_contents] = (current_last_edited_time, current_path)\n",
    "\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "def find_duplicate_files(starting_directory):\n",
    "    files_seen_already = {}\n",
    "    stack = [starting_directory]\n",
    "\n",
    "    # We'll track tuples of (duplicate_file, original_file)\n",
    "    duplicates = []\n",
    "\n",
    "    while len(stack) > 0:\n",
    "        current_path = stack.pop()\n",
    "\n",
    "        # If it's a directory,\n",
    "        # put the contents in our stack\n",
    "        if os.path.isdir(current_path):\n",
    "            for path in os.listdir(current_path):\n",
    "                full_path = os.path.join(current_path, path)\n",
    "                stack.append(full_path)\n",
    "\n",
    "        # If it's a file\n",
    "        else:\n",
    "            # Get its hash\n",
    "            file_hash = sample_hash_file(current_path)\n",
    "\n",
    "            # Get its last edited time\n",
    "            current_last_edited_time = os.path.getmtime(current_path)\n",
    "\n",
    "            # If we've seen it before\n",
    "            if file_hash in files_seen_already:\n",
    "                existing_last_edited_time, existing_path = files_seen_already[file_hash]\n",
    "                if current_last_edited_time > existing_last_edited_time:\n",
    "                    # Current file is the dupe!\n",
    "                    duplicates.append((current_path, existing_path))\n",
    "                else:\n",
    "                    # Old file is the dupe!\n",
    "                    duplicates.append((existing_path, current_path))\n",
    "                    # But also update files_seen_already to have the new file's info\n",
    "                    files_seen_already[file_hash] = (current_last_edited_time, current_path)\n",
    "\n",
    "            # If it's a new file, throw it in files_seen_already and record its path and last edited time,\n",
    "            # so we can tell later if it's a dupe\n",
    "            else:\n",
    "                files_seen_already[file_hash] = (current_last_edited_time, current_path)\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "def sample_hash_file(path):\n",
    "    num_bytes_to_read_per_sample = 4000\n",
    "    total_bytes = os.path.getsize(path)\n",
    "    hasher = hashlib.sha512()\n",
    "\n",
    "    with open(path, 'rb') as file:\n",
    "        # If the file is too short to take 3 samples, hash the entire file\n",
    "        if total_bytes < num_bytes_to_read_per_sample * 3:\n",
    "            hasher.update(file.read())\n",
    "        else:\n",
    "            num_bytes_between_samples = ((total_bytes - num_bytes_to_read_per_sample * 3) / 2)\n",
    "\n",
    "            # Read first, middle, and last bytes\n",
    "            for offset_multiplier in range(3):\n",
    "                start_of_sample = (offset_multiplier\n",
    "                    * (num_bytes_to_read_per_sample + num_bytes_between_samples))\n",
    "                file.seek(start_of_sample)\n",
    "                sample = file.read(num_bytes_to_read_per_sample)\n",
    "                hasher.update(sample)\n",
    "\n",
    "    return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Complexity\n",
    "\n",
    "Each \"fingerprint\" takes O(1) time and space, so our total time and space costs are O(n)where nnn is the number of files on the file system.\n",
    "\n",
    "If we add the last-minute check to see if two files with the same fingerprints are actually the same files (which we probably should), then in the worst case all the files are the same and we have to read their full contents to confirm this, giving us a runtime that's order of the total size of our files on disc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
